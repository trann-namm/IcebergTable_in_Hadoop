{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/trannam/.ivy2/cache\n",
      "The jars for the packages stored in: /home/trannam/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b2fb4045-e7e2-4107-b269-8420d47cbe74;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.7.1/iceberg-spark-runtime-3.5_2.12-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1!iceberg-spark-runtime-3.5_2.12.jar (208201ms)\n",
      ":: resolution report :: resolve 1899ms :: artifacts dl 208208ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b2fb4045-e7e2-4107-b269-8420d47cbe74\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (41794kB/48ms)\n",
      "24/12/17 08:48:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, regexp_extract,to_date, year, month, dayofmonth\n",
    "\n",
    "warehouse_path = \"hdfs://namenode:9000/iceberg/\"\n",
    "iceberg_spark_jar  = '/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.7.1.jar,/opt/bitnami/spark/jars/mysql-connector-j-8.0.31.jar'\n",
    "catalog_name = \"demo\"\n",
    "\n",
    "# Setup iceberg config\n",
    "conf = SparkConf().setAppName(\"MyApp\") \\\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .set('spark.jars.packages', \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\") \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\\\n",
    "    .set(\"spark.sql.defaultCatalog\", catalog_name) \n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Valuation: string (nullable = true)\n",
      " |-- Date_Joined: date (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Industry: string (nullable = true)\n",
      " |-- Selector_Investors: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read parquet file\n",
    "df = spark.read.parquet(\"hdfs://namenode:9000/iceberg/db/table/data/year=2023/month=8/day=25\")\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+-----------+-------+------+-----------------+---------------------------------------------------------+----+-----+---+\n",
      "|index|Company|Valuation|Date_Joined|Country|City  |Industry         |Selector_Investors                                       |year|month|day|\n",
      "+-----+-------+---------+-----------+-------+------+-----------------+---------------------------------------------------------+----+-----+---+\n",
      "|153  |Zepto  |5        |2023-08-25 |India  |Mumbai|Consumer & Retail|Nexus Venture Partners, Contrary, Global Founders Capital|2023|8    |25 |\n",
      "+-----+-------+---------+-----------+-------+------+-----------------+---------------------------------------------------------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO db.table \n",
    "        VALUES (1000,'ByteDance', 225, to_date(2003-04-04), 'China', 'Beijing', 'Media & Entertainment', \n",
    "        'Sequoia Capital China, SIG Asia Investments, Sina Weibo, SoftBank Group',2003,4,4)\"\"\").show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     302|\n",
      "+--------+\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Count(*) FROM db.table\"\"\").show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
